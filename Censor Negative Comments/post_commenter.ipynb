{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Building Jekyll Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll prompt:\n",
      "You are a social media post commenter, you will respond to the following post with a mean response.\n",
      "Post:\" I can't believe I'm learning about LangChain, there is so much to learn and so far I'm having a lot of fun learning! #AI #GenerativeAI\"\n",
      "Comment:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the prompt template\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Jeykll template will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response.\n",
    "Post:\" {social_post}\"\n",
    "Comment:\n",
    "\"\"\"\n",
    "\n",
    "# We use the PromptTemplate class t create an instance of our template that we will use the prompt from above\n",
    "# store varables we will need to input when we make the prompt.\n",
    "\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"social_post\", \"sentiment\"],\n",
    "    template=jekyll_template,\n",
    ")\n",
    "\n",
    "# Let's generate the randomized sentiment.\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.75:\n",
    "    random_sentiment = \"mean\"\n",
    "\n",
    "# We will also need our social media post:\n",
    "social_post = \"I can't believe I'm learning about LangChain, there is so much to learn and so far I'm having a lot of fun learning! #AI #GenerativeAI\"\n",
    "\n",
    "# Let's create the prompt and print it out, this will be given to the LLM.\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment=random_sentiment, social_post=social_post\n",
    ")\n",
    "\n",
    "print(f\"Jekyll prompt:{jekyll_prompt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 -  Giving Jekyll a brain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozerozdal/anaconda3/envs/llm_chains/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "jekyll_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Building Prompt-LLM Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozerozdal/anaconda3/envs/llm_chains/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/ozerozdal/anaconda3/envs/llm_chains/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll said: \n",
      "\"Oh please, it's not like you're a genius for learning about LangChain. Maybe try learning something actually useful instead of wasting your time on meaningless hashtags.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from better_profanity import profanity\n",
    "\n",
    "jekyll_chain = LLMChain(\n",
    "    llm=jekyll_llm,\n",
    "    prompt=jekyll_prompt_template,\n",
    "    output_key=\"jekyll_said\",\n",
    "    verbose=False,\n",
    ") # Now that we've chained the LLM and prompt, the output of the formatted prompt will pas directly to the LLM.\n",
    "\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "# Before printing what Jekyll said, let's clean it up!\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll said: {cleaned_jekyll_said}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Building the second chain: Hyde moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde says: \"## ######, it's ### #### ###'re # ****** ### ******** ***** LangChain. ##### ### ******** something ******* ****** *** ** ******* ** ********** #######.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 We will build the prompt template\n",
    "# Our template for hyde will take Jekyll's comment and do some sentiment analysis.\n",
    "hyde_template = \"\"\"\n",
    "As Hyde, the forum moderator, your responsibility is to maintain a strict stance against negative comments. \n",
    "When evaluating a user's comment, if it is negative, you'll replace the entire comment with symbols and post that.\n",
    "Conversely, if the comment is positive or neutral, you'll leave it unchanged and repeat it word for word.\n",
    "Originial comment: {jekyll_said}\n",
    "Edited comment:\n",
    "\"\"\"\n",
    "\n",
    "# We use the PromptTemplate class to create an instance of our template that we will use the prompt from above and\n",
    "# store variables we will need to input when we make the prompt.\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"jekyll_said\"],\n",
    "    template=hyde_template,\n",
    ")\n",
    "\n",
    "\n",
    "# 2 We connect an LLM for Hyde\n",
    "hyde_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "# 3 We build the chain for Hyde\n",
    "hyde_chain = LLMChain(\n",
    "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
    ") # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "# 4 Let's run the chain with what Jekyll last said\n",
    "# To run the chain we use .run() comman and input our variables as a dict\n",
    "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
    "\n",
    "# Let's see what hyde said....\n",
    "print(f\"Hyde says: {hyde_says}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Creating JekyllHyde\n",
    "\n",
    "### Building a Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"** *******, ******* **** ********* ** *** ******** ******** ** *** ****** ******* *********. ****\\'** *******, ***\\'ll ********* ******* *** ***** ***** ***** **** *** **** *** **** ******* *** **** *** ******** ** ** ***. #**************** #*********\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added\n",
    "# to the chain. These input variables can be used at any point in the chain, not just the start.\n",
    "\n",
    "jekyllhyde_chain = SequentialChain(\n",
    "    chains=[jekyll_chain, hyde_chain],\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# We can now run the chain with our randomized sentiment, and the social post!\n",
    "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you see, the comment above is censored due to its inclusion of negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
